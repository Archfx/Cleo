#!/usr/bin/env python3

import numpy as np
from scipy.stats import entropy

def jensen_shannon_divergence(p, q):
    """
    Compute the Jensen-Shannon Divergence between two probability distributions.
    
    Parameters:
    - p, q: 1-D numpy arrays representing probability distributions.
    
    Returns:
    - Jensen-Shannon Divergence.
    """
    # Normalize distributions to ensure they sum to 1
    p = p / np.sum(p)
    q = q / np.sum(q)
    
    # Calculate the average distribution
    m = 0.5 * (p + q)
    
    # Compute the Jensen-Shannon Divergence
    jsd = 0.5 * (entropy(p, m) + entropy(q, m))
    
    return jsd

# Example usage:
if __name__ == "__main__":
    # Example probability distributions (replace with your own data)
    distribution1 = np.array([1, 1, 1])
    distribution2 = np.array([1, 1999, 1])
    
    # Compute Jensen-Shannon Divergence
    jsd_value = jensen_shannon_divergence(distribution1, distribution2)
    
    print(f"Jensen-Shannon Divergence: {jsd_value}")
